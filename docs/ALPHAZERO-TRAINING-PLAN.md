# AlphaZero è®­ç»ƒæ–¹æ¡ˆ - 2048 æ¸¸æˆ

## æ¦‚è¿°

AlphaZero æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡**è‡ªæˆ‘å¯¹å¼ˆ**å’Œ**è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰**å®ç°æ™ºèƒ½ä½“çš„æŒç»­æ”¹è¿›ã€‚ä¸åŒäºç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆä»ä¸“å®¶æ•°æ®å­¦ä¹ ï¼‰ï¼ŒAlphaZero é€šè¿‡è‡ªæˆ‘åšå¼ˆæ¢ç´¢å’Œå­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œç†è®ºä¸Šå¯ä»¥è¶…è¶Šäººç±»/ç®—æ³•ä¸“å®¶æ°´å¹³ã€‚

### AlphaZero vs ç›‘ç£å­¦ä¹ 

| ç‰¹æ€§ | ç›‘ç£å­¦ä¹  (Method 3) | AlphaZero |
|------|-------------------|-----------|
| æ•°æ®æ¥æº | ä¸“å®¶æ¼”ç¤ºï¼ˆExpectimaxï¼‰ | è‡ªæˆ‘å¯¹å¼ˆ |
| æ€§èƒ½ä¸Šé™ | å—é™äºä¸“å®¶æ°´å¹³ | å¯è¶…è¶Šä¸“å®¶ |
| è®­ç»ƒæ—¶é—´ | ä¸­ç­‰ï¼ˆ200 epochsï¼‰ | é•¿ï¼ˆæ•°ç™¾æ¬¡è¿­ä»£ï¼‰ |
| æ¢ç´¢èƒ½åŠ› | æ—  | é€šè¿‡ MCTS æ¢ç´¢ |
| è®­ç»ƒæ•°æ®è´¨é‡ | é«˜ï¼ˆä¸“å®¶çº§ï¼‰ | é€æ­¥æå‡ |

---

## AlphaZero æ ¸å¿ƒç»„ä»¶

### 1. ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆDual Networkï¼‰

```
è¾“å…¥: (batch, 1, 4, 4) - æ£‹ç›˜çŠ¶æ€

å…±äº«ä¸»å¹²ï¼ˆShared Backboneï¼‰:
  ResBlock 1:
    - Conv2d(1 â†’ 128, kernel=3, padding=1)
    - BatchNorm2d(128)
    - ReLU
    - Conv2d(128 â†’ 128, kernel=3, padding=1)
    - BatchNorm2d(128)
    - Skip connection + ReLU
  
  ResBlock 2:
    - Conv2d(128 â†’ 256, kernel=3, padding=1)
    - BatchNorm2d(256)
    - ReLU
    - Conv2d(256 â†’ 256, kernel=3, padding=1)
    - BatchNorm2d(256)
    - Skip connection (1Ã—1 conv) + ReLU
  
  ResBlock 3-4: åŒä¸Š

ç­–ç•¥å¤´ï¼ˆPolicy Headï¼‰:
  - Flatten â†’ Linear(4096 â†’ 256)
  - ReLU + Dropout(0.3)
  - Linear(256 â†’ 4)
  - è¾“å‡º: (batch, 4) - åŠ¨ä½œæ¦‚ç‡ logits

ä»·å€¼å¤´ï¼ˆValue Headï¼‰:
  - Flatten â†’ Linear(4096 â†’ 256)
  - ReLU + Dropout(0.3)
  - Linear(256 â†’ 128) â†’ ReLU
  - Linear(128 â†’ 1) â†’ Tanh
  - è¾“å‡º: (batch, 1) - å±€é¢ä»·å€¼ [-1, 1]
```

**ç½‘ç»œè¾“å‡º**ï¼š
- **ç­–ç•¥ Ï€(a|s)**ï¼š4 ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼ˆç»è¿‡ softmaxï¼‰
- **ä»·å€¼ v(s)**ï¼šå½“å‰å±€é¢çš„èƒœç‡ä¼°è®¡ï¼ˆ-1 = å¿…è¾“ï¼Œ+1 = å¿…èƒœï¼‰

---

### 2. è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰

MCTS ç”¨äº**å¢å¼ºç¥ç»ç½‘ç»œçš„ç­–ç•¥**ï¼Œé€šè¿‡æ¨¡æ‹Ÿå¯¹å¼ˆæ¥è¯„ä¼°ä¸åŒåŠ¨ä½œçš„é•¿æœŸä»·å€¼ã€‚

#### MCTS å››ä¸ªé˜¶æ®µ

1. **é€‰æ‹©ï¼ˆSelectionï¼‰**
   - ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œé€‰æ‹© UCB å€¼æœ€é«˜çš„å­èŠ‚ç‚¹
   - UCB å…¬å¼ï¼š`UCB = Q(s,a) + c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))`
     - `Q(s,a)`: å¹³å‡ä»·å€¼ï¼ˆç´¯ç§¯ä»·å€¼ / è®¿é—®æ¬¡æ•°ï¼‰
     - `P(s,a)`: ç¥ç»ç½‘ç»œé¢„æµ‹çš„ç­–ç•¥å…ˆéªŒ
     - `N(s)`: çˆ¶èŠ‚ç‚¹è®¿é—®æ¬¡æ•°
     - `N(s,a)`: åŠ¨ä½œ a çš„è®¿é—®æ¬¡æ•°
     - `c_puct`: æ¢ç´¢å¸¸æ•°ï¼ˆé€šå¸¸ 1.0-2.0ï¼‰

2. **æ‰©å±•ï¼ˆExpansionï¼‰**
   - åˆ°è¾¾å¶å­èŠ‚ç‚¹åï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¯„ä¼°ï¼š
     - è·å–ç­–ç•¥å…ˆéªŒ `P(s,a)` ç”¨äºæ‰€æœ‰åˆæ³•åŠ¨ä½œ
     - è·å–ä»·å€¼ä¼°è®¡ `v(s)`
   - åˆ›å»ºæ‰€æœ‰åˆæ³•åŠ¨ä½œçš„å­èŠ‚ç‚¹

3. **è¯„ä¼°ï¼ˆEvaluationï¼‰**
   - ä½¿ç”¨ç¥ç»ç½‘ç»œçš„ä»·å€¼è¾“å‡º `v(s)` ä½œä¸ºå¶å­èŠ‚ç‚¹çš„è¯„ä¼°

4. **å›æº¯ï¼ˆBackpropagationï¼‰**
   - å°†ä»·å€¼ `v` æ²¿ç€æœç´¢è·¯å¾„å‘ä¸Šå›ä¼ 
   - æ›´æ–°æ¯ä¸ªèŠ‚ç‚¹çš„è®¿é—®è®¡æ•° `N` å’Œç´¯ç§¯ä»·å€¼ `W`
   - `Q(s,a) = W(s,a) / N(s,a)`

#### 2048 æ¸¸æˆçš„ MCTS é€‚é…

**æŒ‘æˆ˜**ï¼š2048 æ˜¯**å•äººæ¸¸æˆ + éšæœºæ€§**ï¼ˆæ–°ç –å—éšæœºå‡ºç°ï¼‰ï¼Œä¸åŒäºå›´æ£‹çš„åŒäººå®Œå…¨ä¿¡æ¯åšå¼ˆã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
- **ç©å®¶èŠ‚ç‚¹ï¼ˆMax èŠ‚ç‚¹ï¼‰**ï¼šé€‰æ‹© 4 ä¸ªç§»åŠ¨æ–¹å‘ä¹‹ä¸€
- **æœºä¼šèŠ‚ç‚¹ï¼ˆChance èŠ‚ç‚¹ï¼‰**ï¼šæ¨¡æ‹Ÿéšæœºç –å—ç”Ÿæˆ
  - 90% æ¦‚ç‡ç”Ÿæˆ 2
  - 10% æ¦‚ç‡ç”Ÿæˆ 4
  - åœ¨æ‰€æœ‰ç©ºä½ç½®ä¸­éšæœºé€‰æ‹©

**MCTS æ ‘ç»“æ„**ï¼š
```
          æ ¹èŠ‚ç‚¹ï¼ˆç©å®¶ï¼‰
         /    |    \    \
       ä¸Š    ä¸‹    å·¦    å³  â† ç©å®¶åŠ¨ä½œ
       |     |     |     |
    æœºä¼š  æœºä¼š  æœºä¼š  æœºä¼š   â† éšæœºç –å—
    / \   / \   / \   / \
   2   4 2   4 2   4 2   4  â† ç –å—å€¼
   |   |  |   |  |   |  |   |
  ç©å®¶ ...              ...  â† ç»§ç»­æœç´¢
```

**ä»·å€¼å›ä¼ **ï¼š
- ç©å®¶èŠ‚ç‚¹ï¼šå–**æœ€å¤§å€¼**ï¼ˆé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼‰
- æœºä¼šèŠ‚ç‚¹ï¼šå–**æœŸæœ›å€¼**ï¼ˆ0.9 * v_2 + 0.1 * v_4ï¼Œæ‰€æœ‰ä½ç½®å¹³å‡ï¼‰

---

### 3. è‡ªæˆ‘å¯¹å¼ˆï¼ˆSelf-Playï¼‰

**æµç¨‹**ï¼š
1. åˆå§‹åŒ–ç©ºæ£‹ç›˜ï¼Œéšæœºæ”¾ç½®ä¸¤ä¸ªç –å—
2. å¯¹æ¯ä¸ªå›åˆï¼š
   - è¿è¡Œ MCTSï¼ˆä¾‹å¦‚ 100-800 æ¬¡æ¨¡æ‹Ÿï¼‰
   - æ ¹æ® MCTS è®¿é—®è®¡æ•°ç”Ÿæˆæ”¹è¿›çš„ç­–ç•¥ Ï€_MCTS
   - ä½¿ç”¨æ¸©åº¦å‚æ•°é‡‡æ ·åŠ¨ä½œï¼ˆæ—©æœŸæ¢ç´¢ï¼ŒåæœŸåˆ©ç”¨ï¼‰
   - æ‰§è¡ŒåŠ¨ä½œï¼Œéšæœºç”Ÿæˆæ–°ç –å—
   - è®°å½• (s_t, Ï€_t, z_t) åˆ°è®­ç»ƒæ•°æ®
     - `s_t`: æ£‹ç›˜çŠ¶æ€
     - `Ï€_t`: MCTS æ”¹è¿›çš„ç­–ç•¥ï¼ˆè®¿é—®è®¡æ•°åˆ†å¸ƒï¼‰
     - `z_t`: æ¸¸æˆç»“æœï¼ˆç¨åå¡«å……ï¼‰
3. æ¸¸æˆç»“æŸåï¼š
   - è®¡ç®—ç»“æœ `z`ï¼š
     - `+1` å¦‚æœè¾¾åˆ° 2048ï¼ˆè·èƒœï¼‰
     - `0` å¦‚æœè¾¾åˆ° 1024/512ï¼ˆä¸­ç­‰ï¼‰
     - `-1` å¦‚æœåªè¾¾åˆ° 256 æˆ–æ›´ä½ï¼ˆå¤±è´¥ï¼‰
     - æˆ–ä½¿ç”¨å½’ä¸€åŒ–åˆ†æ•°ï¼š`(score - mean) / std`
4. å°† `z` å¡«å……åˆ°æ‰€æœ‰æ­¥éª¤çš„ `z_t`

**æ¸©åº¦å‚æ•°**ï¼š
- **å‰ 30 æ­¥**ï¼š`Ï„ = 1.0`ï¼ˆæ¢ç´¢ï¼Œå¢åŠ éšæœºæ€§ï¼‰
- **åç»­æ­¥éª¤**ï¼š`Ï„ â†’ 0`ï¼ˆåˆ©ç”¨ï¼Œé€‰æ‹©æœ€ä½³åŠ¨ä½œï¼‰
- åŠ¨ä½œé‡‡æ ·ï¼š`Ï€_sample(a) âˆ N(a)^(1/Ï„)`

**æ•°æ®å¢å¼º**ï¼š
- 8 é‡å¯¹ç§°æ€§ï¼ˆ4 æ—‹è½¬ Ã— 2 ç¿»è½¬ï¼‰
- è®­ç»ƒæ•°æ®å¢åŠ  8 å€

---

### 4. è®­ç»ƒå¾ªç¯

**æŸå¤±å‡½æ•°**ï¼š
```python
total_loss = (z - v)^2 - Ï€^T * log(p) + c * ||Î¸||^2
```
- `(z - v)^2`: ä»·å€¼æŸå¤±ï¼ˆMSEï¼‰
- `-Ï€^T * log(p)`: ç­–ç•¥æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
- `c * ||Î¸||^2`: L2 æ­£åˆ™åŒ–

**è®­ç»ƒæµç¨‹**ï¼š
1. ä»ç»éªŒæ± ä¸­é‡‡æ · mini-batchï¼ˆä¾‹å¦‚ 256 samplesï¼‰
2. è®¡ç®—æŸå¤±å¹¶æ›´æ–°ç½‘ç»œå‚æ•°
3. å®šæœŸè¯„ä¼°å½“å‰ç½‘ç»œ vs æœ€ä½³ç½‘ç»œ
4. å¦‚æœæ–°ç½‘ç»œèƒœç‡ > 55%ï¼Œæ›´æ–°æœ€ä½³ç½‘ç»œ

**è¶…å‚æ•°**ï¼š
- Batch size: 256
- Learning rate: 0.001ï¼ˆåˆå§‹ï¼‰ï¼ŒåŠ¨æ€è°ƒæ•´
- Optimizer: SGD with momentum 0.9
- Weight decay: 1e-4
- Training epochs per iteration: 10-20

---

## AlphaZero è®­ç»ƒæµç¨‹

### ç¬¬ 0 é˜¶æ®µï¼šåˆå§‹åŒ–ï¼ˆBootstrapï¼‰

**é€‰é¡¹ 1ï¼šä»é›¶å¼€å§‹**
- ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ç½‘ç»œ
- å‰ 10-20 å±€æ¸¸æˆè´¨é‡è¾ƒå·®
- éœ€è¦æ›´å¤šè¿­ä»£æ‰èƒ½æ”¶æ•›

**é€‰é¡¹ 2ï¼šä»ç›‘ç£å­¦ä¹ åˆå§‹åŒ–ï¼ˆæ¨èï¼‰**
- ä½¿ç”¨å·²è®­ç»ƒçš„ transformer/dual network
- å¿«é€Ÿå¯åŠ¨ï¼Œå‡å°‘è¿­ä»£æ¬¡æ•°
- æ›´å¿«è¾¾åˆ°è¶…è¶Šä¸“å®¶æ°´å¹³

**å†³ç­–**ï¼šä½¿ç”¨å·²æœ‰çš„ transformer æ¨¡å‹ä½œä¸ºåˆå§‹åŒ–ï¼ˆbest_model.pthï¼‰ï¼Œè½¬æ¢ä¸º ResNet æ¶æ„ã€‚

---

### ç¬¬ 1 é˜¶æ®µï¼šMCTS å®ç°

**æ–‡ä»¶**ï¼š`training/mcts.py`

**æ ¸å¿ƒç±»**ï¼š
```python
class MCTSNode:
    """MCTS æ ‘èŠ‚ç‚¹"""
    def __init__(self, state, parent=None, action=None, prior=0.0):
        self.state = state           # æ£‹ç›˜çŠ¶æ€
        self.parent = parent         # çˆ¶èŠ‚ç‚¹
        self.action = action         # åˆ°è¾¾æ­¤èŠ‚ç‚¹çš„åŠ¨ä½œ
        self.prior = prior           # ç­–ç•¥å…ˆéªŒ P(s,a)
        self.children = {}           # å­èŠ‚ç‚¹ {action: node}
        self.visit_count = 0         # N(s,a)
        self.total_value = 0.0       # W(s,a)
        self.is_chance_node = False  # æ˜¯å¦ä¸ºæœºä¼šèŠ‚ç‚¹
    
    def q_value(self):
        """å¹³å‡ä»·å€¼ Q(s,a)"""
        if self.visit_count == 0:
            return 0.0
        return self.total_value / self.visit_count
    
    def ucb_score(self, c_puct=1.4):
        """UCB åˆ†æ•°"""
        if self.visit_count == 0:
            return float('inf')
        exploration = c_puct * self.prior * np.sqrt(self.parent.visit_count) / (1 + self.visit_count)
        return self.q_value() + exploration

class MCTS:
    """è’™ç‰¹å¡æ´›æ ‘æœç´¢"""
    def __init__(self, model, num_simulations=100, c_puct=1.4):
        self.model = model
        self.num_simulations = num_simulations
        self.c_puct = c_puct
    
    def search(self, state):
        """æ‰§è¡Œ MCTS æœç´¢ï¼Œè¿”å›æ”¹è¿›çš„ç­–ç•¥"""
        root = MCTSNode(state)
        
        for _ in range(self.num_simulations):
            node = root
            # 1. Selection
            while node.children:
                node = self._select_child(node)
            
            # 2. Expansion & Evaluation
            if not is_terminal(node.state):
                value = self._expand_and_evaluate(node)
            else:
                value = self._terminal_value(node.state)
            
            # 3. Backpropagation
            self._backpropagate(node, value)
        
        # è¿”å›è®¿é—®è®¡æ•°åˆ†å¸ƒä½œä¸ºæ”¹è¿›ç­–ç•¥
        return self._get_action_probs(root)
    
    def _select_child(self, node):
        """é€‰æ‹© UCB æœ€å¤§çš„å­èŠ‚ç‚¹"""
        return max(node.children.values(), key=lambda n: n.ucb_score(self.c_puct))
    
    def _expand_and_evaluate(self, node):
        """æ‰©å±•èŠ‚ç‚¹å¹¶è¯„ä¼°"""
        policy, value = self.model.predict(node.state)
        
        # åˆ›å»ºæ‰€æœ‰åˆæ³•åŠ¨ä½œçš„å­èŠ‚ç‚¹
        for action in range(4):
            if is_valid_action(node.state, action):
                # åˆ›å»ºæœºä¼šèŠ‚ç‚¹
                chance_node = MCTSNode(node.state, parent=node, action=action, prior=policy[action])
                chance_node.is_chance_node = True
                node.children[action] = chance_node
        
        return value
    
    def _backpropagate(self, node, value):
        """å‘ä¸Šå›ä¼ ä»·å€¼"""
        while node is not None:
            node.visit_count += 1
            node.total_value += value
            value = -value  # ç¿»è½¬ä»·å€¼ï¼ˆå¯¹æ‰‹è§†è§’ï¼‰
            node = node.parent
```

**å®ç°è¦ç‚¹**ï¼š
- å¤„ç†æœºä¼šèŠ‚ç‚¹ï¼ˆéšæœºç –å—ï¼‰
- é«˜æ•ˆçš„æ£‹ç›˜çŠ¶æ€è¡¨ç¤º
- è™šæ‹ŸæŸå¤±ï¼ˆVirtual Lossï¼‰ç”¨äºå¹¶è¡Œæœç´¢
- Dirichlet å™ªå£°ç”¨äºæ ¹èŠ‚ç‚¹æ¢ç´¢ï¼ˆè®­ç»ƒæ—¶ï¼‰

---

### ç¬¬ 2 é˜¶æ®µï¼šDual Network å®ç°

**æ–‡ä»¶**ï¼š`models/dual/alphazero_network.py`

**ç½‘ç»œç»“æ„**ï¼š
```python
class AlphaZeroNetwork(nn.Module):
    """AlphaZero åŒå¤´ç½‘ç»œ"""
    def __init__(self, num_blocks=4, channels=256):
        super().__init__()
        
        # åˆå§‹å·ç§¯
        self.conv_input = nn.Conv2d(1, channels, kernel_size=3, padding=1)
        self.bn_input = nn.BatchNorm2d(channels)
        
        # ResNet blocks
        self.res_blocks = nn.ModuleList([
            ResBlock(channels) for _ in range(num_blocks)
        ])
        
        # ç­–ç•¥å¤´
        self.policy_conv = nn.Conv2d(channels, 32, kernel_size=1)
        self.policy_bn = nn.BatchNorm2d(32)
        self.policy_fc = nn.Linear(32 * 4 * 4, 4)
        
        # ä»·å€¼å¤´
        self.value_conv = nn.Conv2d(channels, 32, kernel_size=1)
        self.value_bn = nn.BatchNorm2d(32)
        self.value_fc1 = nn.Linear(32 * 4 * 4, 256)
        self.value_fc2 = nn.Linear(256, 1)
    
    def forward(self, x):
        # Shared backbone
        x = F.relu(self.bn_input(self.conv_input(x)))
        for block in self.res_blocks:
            x = block(x)
        
        # Policy head
        p = F.relu(self.policy_bn(self.policy_conv(x)))
        p = p.view(p.size(0), -1)
        p = self.policy_fc(p)  # Logits
        
        # Value head
        v = F.relu(self.value_bn(self.value_conv(x)))
        v = v.view(v.size(0), -1)
        v = F.relu(self.value_fc1(v))
        v = torch.tanh(self.value_fc2(v))
        
        return p, v
```

---

### ç¬¬ 3 é˜¶æ®µï¼šè‡ªæˆ‘å¯¹å¼ˆæ•°æ®ç”Ÿæˆ

**æ–‡ä»¶**ï¼š`training/selfplay.py`

**æ ¸å¿ƒå‡½æ•°**ï¼š
```python
def self_play_game(model, mcts_simulations=100, temperature=1.0):
    """
    ä½¿ç”¨ MCTS è¿›è¡Œä¸€å±€è‡ªæˆ‘å¯¹å¼ˆ
    
    Returns:
        training_examples: List[(state, mcts_policy, value)]
    """
    game = Game2048()
    training_examples = []
    move_count = 0
    
    while not game.game_over and move_count < 5000:
        # è¿è¡Œ MCTS
        mcts = MCTS(model, num_simulations=mcts_simulations)
        mcts_policy = mcts.search(game.get_state())
        
        # è®°å½•è®­ç»ƒæ ·æœ¬ï¼ˆç»“æœç¨åå¡«å……ï¼‰
        training_examples.append({
            'state': game.get_state().clone(),
            'policy': mcts_policy.copy(),
            'value': None  # æ¸¸æˆç»“æŸåå¡«å……
        })
        
        # æ¸©åº¦é‡‡æ ·åŠ¨ä½œ
        tau = 1.0 if move_count < 30 else 0.1
        action = sample_action(mcts_policy, temperature=tau)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        game.move(action)
        move_count += 1
    
    # è®¡ç®—æ¸¸æˆç»“æœ
    max_tile = np.max(game.board)
    if max_tile >= 2048:
        result = 1.0
    elif max_tile >= 1024:
        result = 0.5
    elif max_tile >= 512:
        result = 0.0
    else:
        result = -0.5
    
    # å¡«å……ç»“æœåˆ°æ‰€æœ‰æ­¥éª¤
    for example in training_examples:
        example['value'] = result
    
    return training_examples, {
        'score': game.score,
        'max_tile': max_tile,
        'moves': move_count
    }
```

**å¹¶è¡Œè‡ªæˆ‘å¯¹å¼ˆ**ï¼š
- ä½¿ç”¨å¤šè¿›ç¨‹ç”Ÿæˆæ•°æ®ï¼ˆä¾‹å¦‚ 8 ä¸ªè¿›ç¨‹ï¼‰
- æ¯æ¬¡è¿­ä»£ç”Ÿæˆ 100-500 å±€æ¸¸æˆ
- æ•°æ®å¢å¼ºï¼ˆ8 å€å¯¹ç§°æ€§ï¼‰

---

### ç¬¬ 4 é˜¶æ®µï¼šè®­ç»ƒå¾ªç¯

**æ–‡ä»¶**ï¼š`training/train_alphazero.py`

**ä¸»è®­ç»ƒå¾ªç¯**ï¼š
```python
def train_alphazero(iterations=100, games_per_iteration=100):
    """AlphaZero ä¸»è®­ç»ƒå¾ªç¯"""
    
    # åˆå§‹åŒ–
    model = AlphaZeroNetwork()
    best_model = copy.deepcopy(model)
    replay_buffer = ReplayBuffer(max_size=500000)
    
    for iteration in range(iterations):
        print(f"\n{'='*60}")
        print(f"Iteration {iteration + 1}/{iterations}")
        print(f"{'='*60}")
        
        # 1. è‡ªæˆ‘å¯¹å¼ˆç”Ÿæˆæ•°æ®
        print(f"ğŸ® Self-play: Generating {games_per_iteration} games...")
        new_examples = []
        stats = []
        
        for game_idx in tqdm(range(games_per_iteration)):
            examples, game_stats = self_play_game(
                model, 
                mcts_simulations=100 if iteration < 10 else 200
            )
            new_examples.extend(examples)
            stats.append(game_stats)
        
        # å¢å¼ºå¹¶æ·»åŠ åˆ°ç»éªŒæ± 
        augmented_examples = augment_examples(new_examples)
        replay_buffer.add(augmented_examples)
        
        print(f"  Generated {len(new_examples)} examples (Ã—8 aug = {len(augmented_examples)})")
        print(f"  Avg score: {np.mean([s['score'] for s in stats]):.0f}")
        print(f"  Win rate: {np.mean([s['max_tile'] >= 2048 for s in stats])*100:.1f}%")
        
        # 2. è®­ç»ƒç½‘ç»œ
        print(f"ğŸ”§ Training network...")
        train_metrics = train_network(
            model, 
            replay_buffer, 
            epochs=10, 
            batch_size=256
        )
        
        print(f"  Policy loss: {train_metrics['policy_loss']:.4f}")
        print(f"  Value loss: {train_metrics['value_loss']:.4f}")
        
        # 3. è¯„ä¼°æ–°æ¨¡å‹
        if iteration % 5 == 0:
            print(f"âš”ï¸  Evaluating: New model vs Best model...")
            win_rate = evaluate_models(model, best_model, num_games=50)
            print(f"  New model win rate: {win_rate*100:.1f}%")
            
            if win_rate > 0.55:
                print(f"  âœ“ New model is better! Updating best model.")
                best_model = copy.deepcopy(model)
            else:
                print(f"  âœ— Best model retained.")
        
        # 4. ä¿å­˜æ£€æŸ¥ç‚¹
        if iteration % 10 == 0:
            save_checkpoint({
                'iteration': iteration,
                'model': model.state_dict(),
                'best_model': best_model.state_dict(),
                'replay_buffer': replay_buffer.get_state()
            }, f'checkpoints/alphazero_iter{iteration}.pth')
    
    return best_model
```

---

### ç¬¬ 5 é˜¶æ®µï¼šè¯„ä¼°ä¸å¯¹æ¯”

**å¯¹æ¯”åŸºå‡†**ï¼š
- Expectimax (æ·±åº¦ 4)ï¼š~80% èƒœç‡
- ç›‘ç£å­¦ä¹  Transformerï¼ˆ200 epochsï¼‰ï¼šé¢„æœŸ 70% èƒœç‡
- AlphaZeroï¼ˆ100 æ¬¡è¿­ä»£ï¼‰ï¼šç›®æ ‡ 85%+ èƒœç‡

**è¯„ä¼°æŒ‡æ ‡**ï¼š
1. **èƒœç‡**ï¼šè¾¾åˆ° 2048 çš„æ¯”ä¾‹
2. **å¹³å‡åˆ†æ•°**ï¼šæ‰€æœ‰æ¸¸æˆçš„å¹³å‡å¾—åˆ†
3. **æœ€å¤§ç –å—åˆ†å¸ƒ**ï¼š512/1024/2048/4096
4. **å¯¹æˆ˜èƒœç‡**ï¼švs Expectimax, vs ç›‘ç£æ¨¡å‹

**è¯„ä¼°è„šæœ¬**ï¼š`evaluation/evaluate_alphazero.py`

---

## å®ç°è®¡åˆ’

### é˜¶æ®µ 1ï¼šMCTS å®ç°ï¼ˆç¬¬ 1-2 å¤©ï¼‰
- [ ] å®ç° `MCTSNode` ç±»
- [ ] å®ç° `MCTS` ç±»ï¼ˆé€‰æ‹©ã€æ‰©å±•ã€å›æº¯ï¼‰
- [ ] å¤„ç†æœºä¼šèŠ‚ç‚¹ï¼ˆéšæœºç –å—ï¼‰
- [ ] æµ‹è¯• MCTS ä¸éšæœºç­–ç•¥ç½‘ç»œ

### é˜¶æ®µ 2ï¼šDual Networkï¼ˆç¬¬ 3 å¤©ï¼‰
- [ ] å®ç° `AlphaZeroNetwork`ï¼ˆResNet + åŒå¤´ï¼‰
- [ ] å®ç° `ResBlock`
- [ ] æµ‹è¯•å‰å‘ä¼ æ’­å’Œæ¢¯åº¦æµ
- [ ] è®¡ç®—ç½‘ç»œå‚æ•°é‡

### é˜¶æ®µ 3ï¼šè‡ªæˆ‘å¯¹å¼ˆï¼ˆç¬¬ 4-5 å¤©ï¼‰
- [ ] å®ç° `self_play_game()`
- [ ] å®ç°æ¸©åº¦é‡‡æ ·
- [ ] å®ç° `ReplayBuffer`
- [ ] æµ‹è¯•å¹¶è¡Œè‡ªæˆ‘å¯¹å¼ˆ

### é˜¶æ®µ 4ï¼šè®­ç»ƒå¾ªç¯ï¼ˆç¬¬ 6-7 å¤©ï¼‰
- [ ] å®ç° `train_network()`
- [ ] å®ç° `evaluate_models()`
- [ ] å®ç°ä¸»è®­ç»ƒå¾ªç¯ `train_alphazero()`
- [ ] æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆ10 æ¬¡è¿­ä»£ï¼‰

### é˜¶æ®µ 5ï¼šå®Œæ•´è®­ç»ƒï¼ˆç¬¬ 8-14 å¤©ï¼‰
- [ ] è¿è¡Œ 100 æ¬¡è¿­ä»£
- [ ] ç›‘æ§è®­ç»ƒæ›²çº¿
- [ ] å®šæœŸè¯„ä¼°æ€§èƒ½
- [ ] ä¸ç›‘ç£å­¦ä¹ æ¨¡å‹å¯¹æ¯”

### é˜¶æ®µ 6ï¼šä¼˜åŒ–ä¸åˆ†æï¼ˆç¬¬ 15-16 å¤©ï¼‰
- [ ] è¶…å‚æ•°è°ƒä¼˜ï¼ˆMCTS æ¨¡æ‹Ÿæ¬¡æ•°ã€å­¦ä¹ ç‡ç­‰ï¼‰
- [ ] å¯è§†åŒ– MCTS æœç´¢æ ‘
- [ ] åˆ†æç­–ç•¥æ”¹è¿›è¿‡ç¨‹
- [ ] æ’°å†™ç»“æœæŠ¥å‘Š

---

## è¶…å‚æ•°é…ç½®

### MCTS å‚æ•°
- **æ¨¡æ‹Ÿæ¬¡æ•°ï¼ˆnum_simulationsï¼‰**ï¼š
  - åˆæœŸï¼š100ï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
  - åæœŸï¼š200-400ï¼ˆæ›´ç²¾ç¡®çš„ç­–ç•¥ï¼‰
  - è¯„ä¼°ï¼š800ï¼ˆæœ€ä½³æ€§èƒ½ï¼‰
- **æ¢ç´¢å¸¸æ•°ï¼ˆc_puctï¼‰**ï¼š1.4
- **Dirichlet å™ªå£°**ï¼šÎ±=0.3, Îµ=0.25ï¼ˆè®­ç»ƒæ—¶æ ¹èŠ‚ç‚¹ï¼‰
- **æ¸©åº¦å‚æ•°ï¼ˆtemperatureï¼‰**ï¼š
  - å‰ 30 æ­¥ï¼šÏ„=1.0
  - åç»­æ­¥éª¤ï¼šÏ„=0.1

### ç½‘ç»œæ¶æ„
- **ResNet blocks**ï¼š4-6 å±‚
- **Channels**ï¼š256
- **Dropout**ï¼š0.3ï¼ˆç­–ç•¥/ä»·å€¼å¤´ï¼‰

### è®­ç»ƒå‚æ•°
- **æ¯æ¬¡è¿­ä»£æ¸¸æˆæ•°**ï¼š100-200
- **ç»éªŒæ± å¤§å°**ï¼š500,000 samples
- **Batch size**ï¼š256
- **Learning rate**ï¼š0.001 â†’ 0.0001ï¼ˆè¡°å‡ï¼‰
- **Optimizer**ï¼šSGD with momentum 0.9
- **Weight decay**ï¼š1e-4
- **è®­ç»ƒ epochs/iteration**ï¼š10-20

### è¯„ä¼°å‚æ•°
- **å¯¹æˆ˜æ¸¸æˆæ•°**ï¼š50-100
- **æ›´æ–°é˜ˆå€¼**ï¼š55% èƒœç‡

---

## é¢„æœŸç»“æœ

### çŸ­æœŸç›®æ ‡ï¼ˆ20-30 æ¬¡è¿­ä»£ï¼Œ~3-5 å¤©ï¼‰
- [ ] èƒœç‡ > 50%ï¼ˆè¶…è¿‡éšæœº baselineï¼‰
- [ ] å¹³å‡åˆ†æ•° > 5,000
- [ ] ç¨³å®šçš„ç­–ç•¥æ”¹è¿›æ›²çº¿

### ä¸­æœŸç›®æ ‡ï¼ˆ50-70 æ¬¡è¿­ä»£ï¼Œ~1 å‘¨ï¼‰
- [ ] èƒœç‡ > 70%ï¼ˆæ¥è¿‘ Expectimaxï¼‰
- [ ] å¹³å‡åˆ†æ•° > 12,000
- [ ] è¶…è¶Šç›‘ç£å­¦ä¹ æ¨¡å‹

### é•¿æœŸç›®æ ‡ï¼ˆ100+ æ¬¡è¿­ä»£ï¼Œ~2 å‘¨ï¼‰
- [ ] èƒœç‡ > 85%ï¼ˆè¶…è¶Š Expectimaxï¼‰
- [ ] å¹³å‡åˆ†æ•° > 18,000
- [ ] 10%+ æ¸¸æˆè¾¾åˆ° 4096
- [ ] ç­–ç•¥å…·æœ‰æ˜ç¡®çš„é•¿æœŸè§„åˆ’èƒ½åŠ›

---

## ä¸ç›‘ç£å­¦ä¹ çš„å¯¹æ¯”

| æŒ‡æ ‡ | ç›‘ç£å­¦ä¹ ï¼ˆTransformerï¼‰ | AlphaZero |
|------|----------------------|-----------|
| è®­ç»ƒæ•°æ® | 500 games (Expectimax) | 10,000+ games (self-play) |
| è®­ç»ƒæ—¶é—´ | ~50 å°æ—¶ï¼ˆ200 epochsï¼‰ | ~100-200 å°æ—¶ï¼ˆ100 iterationsï¼‰ |
| æ€§èƒ½ä¸Šé™ | ~75% èƒœç‡ï¼ˆå—é™äºä¸“å®¶ï¼‰ | 85%+ èƒœç‡ï¼ˆæŒç»­æ”¹è¿›ï¼‰ |
| æ³›åŒ–èƒ½åŠ› | ä¸­ç­‰ | å¼ºï¼ˆæ¢ç´¢æ›´å¤šçŠ¶æ€ï¼‰ |
| æ•°æ®æ•ˆç‡ | é«˜ | ä½ï¼ˆéœ€è¦å¤§é‡è‡ªæˆ‘å¯¹å¼ˆï¼‰ |
| å®ç°å¤æ‚åº¦ | ä¸­ | é«˜ï¼ˆMCTS + è®­ç»ƒå¾ªç¯ï¼‰ |

---

## æŠ€æœ¯æŒ‘æˆ˜

### 1. éšæœºæ€§å¤„ç†
- **é—®é¢˜**ï¼š2048 æœ‰éšæœºç –å—ç”Ÿæˆï¼Œä¸åŒäºå›´æ£‹
- **è§£å†³**ï¼šæœºä¼šèŠ‚ç‚¹ + æœŸæœ›å€¼å›ä¼ 

### 2. ä»·å€¼è¯„ä¼°
- **é—®é¢˜**ï¼šæ¸¸æˆç»“æœä¸æ˜¯äºŒå…ƒï¼ˆèƒœ/è´Ÿï¼‰
- **è§£å†³**ï¼šå½’ä¸€åŒ–åˆ†æ•°æˆ–åˆ†çº§å¥–åŠ±ï¼ˆ2048=+1, 1024=+0.5, etc.ï¼‰

### 3. è®­ç»ƒæ—¶é—´
- **é—®é¢˜**ï¼šè‡ªæˆ‘å¯¹å¼ˆ + MCTS è®¡ç®—é‡å¤§
- **è§£å†³**ï¼š
  - å¹¶è¡Œè‡ªæˆ‘å¯¹å¼ˆï¼ˆå¤šè¿›ç¨‹ï¼‰
  - é™ä½åˆæœŸ MCTS æ¨¡æ‹Ÿæ¬¡æ•°
  - GPU åŠ é€Ÿç½‘ç»œæ¨ç†

### 4. æ¢ç´¢ vs åˆ©ç”¨
- **é—®é¢˜**ï¼šæ—©æœŸæ¨¡å‹è´¨é‡å·®ï¼Œéœ€è¦æ¢ç´¢
- **è§£å†³**ï¼š
  - Dirichlet å™ªå£°å¢åŠ æ ¹èŠ‚ç‚¹æ¢ç´¢
  - æ¸©åº¦å‚æ•°æ§åˆ¶é‡‡æ ·éšæœºæ€§
  - ç»éªŒæ± ä¿ç•™å†å²æ•°æ®

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **åˆ›å»ºç›®å½•ç»“æ„**ï¼š
   ```
   models/dual/
   training/mcts.py
   training/selfplay.py
   training/train_alphazero.py
   evaluation/evaluate_alphazero.py
   ```

2. **å®ç° MCTS**ï¼šä»ç®€å•ç‰ˆæœ¬å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–

3. **å®ç° Dual Network**ï¼šåŸºäº ResNet æ¶æ„

4. **æµ‹è¯•è‡ªæˆ‘å¯¹å¼ˆ**ï¼šç¡®ä¿æ•°æ®ç”Ÿæˆæ­£ç¡®

5. **å°è§„æ¨¡è®­ç»ƒ**ï¼š10 æ¬¡è¿­ä»£éªŒè¯æµç¨‹

6. **å®Œæ•´è®­ç»ƒ**ï¼š100+ æ¬¡è¿­ä»£è¾¾åˆ°æœ€ä¼˜æ€§èƒ½

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼š1.0  
**åˆ›å»ºæ—¥æœŸ**ï¼š2026-01-08  
**çŠ¶æ€**ï¼šå¾…å®ç°

**å‚è€ƒèµ„æº**ï¼š
- AlphaGo Zero è®ºæ–‡ï¼šMastering the game of Go without human knowledge
- AlphaZero è®ºæ–‡ï¼šA general reinforcement learning algorithm that masters chess, shogi, and Go through self-play
- 2048 Expectimax å®ç°ï¼šscripts/generate_dataset.py
